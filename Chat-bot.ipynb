{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat-bot in the Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_config.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a0616a1da1f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_config.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mbig_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#big_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_config.txt'"
     ]
    }
   ],
   "source": [
    "with open('final_config.txt', 'r', encoding='utf-8') as f:\n",
    "    big_config = f.read()\n",
    "    f.close()\n",
    "#big_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'big_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-33aca0b9ffa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# убираем мусор из текстового файла\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;34m'\\ufeff'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbig_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mbig_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbig_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\ufeff'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# преобразуем text в json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbig_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'big_config' is not defined"
     ]
    }
   ],
   "source": [
    "# убираем мусор из текстового файла\n",
    "if '\\ufeff' in big_config:\n",
    "        big_config = big_config.split('\\ufeff')[1]\n",
    "# преобразуем text в json\n",
    "big_config = eval(big_config)\n",
    "#big_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# набор данных на вход и на выход\n",
    "\n",
    "BOT_CONFIG = big_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вытащим интенты и их значения(example)\n",
    "dataset = []\n",
    "\n",
    "for intent, intent_data in BOT_CONFIG['intents'].items():\n",
    "    for example in intent_data['examples']:\n",
    "        dataset.append([example, intent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаём объекты\n",
    "X_text = [x for x, y in dataset]\n",
    "# задаём соответствующие им классы\n",
    "y = [y for x, y in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключаем библиотеку для векторизации\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3)) \n",
    "X = vectorizer.fit_transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подключаем библиотеку для обучения модели\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Так как проверка прошла успешно, поэтому обучаем на всём наборе данных\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(text):\n",
    "    # Список вероятностей попадания вопроса в каждый класс\n",
    "    probas = clf.predict_proba(vectorizer.transform([text]))[0]\n",
    "    # ищем максимальную вероятность попадания\n",
    "    proba = max(probas)\n",
    "    # и проверяем её на пригодность\n",
    "    if proba > 0.3:\n",
    "        # определяем индекс класса, к которому принадлежит ответ\n",
    "        index = list(probas).index(proba)\n",
    "        # возвращаем данный класс\n",
    "        return clf.classes_[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dialogues.txt', 'r', encoding='utf-8') as f:\n",
    "    dialogues_data = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# составляем общий список диалогов, убираем абзацы и переносы на следующую строку\n",
    "dialogues = [dialogue.split('\\n')[:2] for dialogue in dialogues_data.split('\\n\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставляем только те диалоги, которые состоят из двух реплик\n",
    "dialogues = [dialogue for dialogue in dialogues if len(dialogue) ==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues_filtered = []\n",
    "# убираем мусор, кроме тех символов, которые нужны\n",
    "alphabet = '1234567890- абвгдеёжзийклмнопрстуфхцчшщъыьэюяqwertyuiopasdfghjklzxcvbnm'\n",
    "\n",
    "for dialogue in dialogues:\n",
    "    # Пропускаем первые два символа в реплике(- ) и убираем заглавную букву\n",
    "    question = dialogue[0][2:].lower()\n",
    "    # Оставляем в цикле реплики, которые состоят из нужных символов, и соединяем их в предложение с помощью join в tuple\n",
    "    question = ''.join(char for char in question if char in alphabet)\n",
    "    # убираем пробелы до и после реплики\n",
    "    question = question.strip()\n",
    "    # убираем пробелы до и после реплики\n",
    "    answer = dialogue[1][2:].strip()\n",
    "    if question and answer:\n",
    "        dialogues_filtered.append((question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убираем повторы\n",
    "dialogues_filtered = list(set(dialogues_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для оптимизации поиска правильного ответа, было решено разбить вопросы на ключевые слова\n",
    "search_structure = {}  # {word: [(q, a), (q, a), ...], ...}\n",
    "\n",
    "for question, answer in dialogues_filtered:\n",
    "    # разбиваем вопрос на ключевые слова\n",
    "    words = question.split(' ')\n",
    "    for word in words:\n",
    "        if word not in search_structure:\n",
    "            search_structure[word] = []\n",
    "        search_structure[word].append((question, answer))\n",
    "\n",
    "# Чтобы не было путаницы между популярными ключевыми словами, было решено их исключить\n",
    "to_del = []\n",
    "for word in search_structure:\n",
    "    if len(search_structure[word]) > 10000:\n",
    "        to_del.append(word)\n",
    "\n",
    "# Удаление ключевого слова, которое слишком популярно\n",
    "for word in to_del:\n",
    "    search_structure.pop(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generative_response(text):\n",
    "    # Убираем заглавие\n",
    "    text = text.lower()\n",
    "     # Оставляем в цикле реплики, которые состоят из нужных символов, и соединяем их в предложение с помощью join в tuple\n",
    "    text = ''.join(char for char in text if char in alphabet)\n",
    "     # убираем пробелы до и после реплики\n",
    "    text = text.strip()\n",
    "     # разбиваем вопрос на ключевые слова\n",
    "    words = text.split(' ')\n",
    "\n",
    "    # Для ускорения поиска ответа было решено искать ответы, которые подходят по ключевым словам в вопросе\n",
    "    qas = []\n",
    "    for word in words:\n",
    "        if word in search_structure:\n",
    "            qas += search_structure[word]\n",
    "            \n",
    "   # Для ускорения выдачи ответа было решено ограничить порог входа с помощью расстояния Левенштейна\n",
    "    for question, answer in qas:\n",
    "        if abs(len(text) - len(question)) < len(question) * 0.20:\n",
    "            edit_distance = nltk.edit_distance(text, question)\n",
    "            if edit_distance / len(question) < 0.20:\n",
    "                return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбор ответа по закону рандома на фразы, которые есть в наборе данных\n",
    "def get_response_by_intent(intent):\n",
    "    phrases = BOT_CONFIG['intents'][intent]['responses']\n",
    "    return random.choice(phrases)\n",
    "\n",
    "# выбор ответа по закону рандома на фразы, которых нет в наборе данных \n",
    "def get_failure_phrases():\n",
    "    phrases = BOT_CONFIG['failure_phrases']\n",
    "    return random.choice(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистика использования моделей(функций)\n",
    "stats = {'intent': 0, 'generative': 0, 'stub': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot(text):   \n",
    "    \"\"\"Генерация ответной реплики\"\"\"\n",
    "    \n",
    "    # NLU - распознавание\n",
    "    intent = get_intent(text)\n",
    "    # rules\n",
    "    if intent:\n",
    "        stats['intent'] += 1\n",
    "        return get_response_by_intent(intent)\n",
    "\n",
    "    # use generative model\n",
    "    response = get_generative_response(text)\n",
    "    # rules\n",
    "    if response:\n",
    "        stats['generative'] += 1\n",
    "        return response\n",
    "    \n",
    "    # Generate answer\n",
    "    \n",
    "    # stub - если нет ни одного ответа!\n",
    "    stats['stub'] += 1\n",
    "    return get_failure_phrases() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-telegram-bot in c:\\users\\admin\\anaconda3\\lib\\site-packages (12.8)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-telegram-bot) (6.0.3)\n",
      "Requirement already satisfied: cryptography in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-telegram-bot) (2.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-telegram-bot) (2019.11.28)\n",
      "Requirement already satisfied: decorator>=4.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-telegram-bot) (4.4.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cryptography->python-telegram-bot) (1.14.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cryptography->python-telegram-bot) (1.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot) (2.19)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/python-telegram-bot/python-telegram-bot\n",
    "! pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/python-telegram-bot/python-telegram-bot/blob/master/examples/echobot.py\n",
    "\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
    "\n",
    "\n",
    "def start(update, context):\n",
    "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
    "    update.message.reply_text('Hi!')\n",
    "\n",
    "\n",
    "def help_command(update, context):\n",
    "    \"\"\"Send a message when the command /help is issued.\"\"\"\n",
    "    update.message.reply_text('Help!')\n",
    "\n",
    "\n",
    "def bot_answer(update, context):\n",
    "    \"\"\"Echo the user message.\"\"\"\n",
    "    question = update.message.text\n",
    "    answer = bot(question)\n",
    "    print(question, answer)\n",
    "    print(stats)\n",
    "    print()\n",
    "    update.message.reply_text(answer)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Start the bot.\"\"\"\n",
    "    # добавляем свой token\n",
    "    updater = Updater(\"***********************************************\", use_context=True)\n",
    "\n",
    "    dp = updater.dispatcher\n",
    "    dp.add_handler(CommandHandler(\"start\", start))\n",
    "    dp.add_handler(CommandHandler(\"help\", help_command))\n",
    "    dp.add_handler(MessageHandler(Filters.text & ~Filters.command, bot_answer))\n",
    "\n",
    "    # Start the Bot\n",
    "    updater.start_polling()\n",
    "    updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очистить Не поняла вас\n",
      "{'intent': 0, 'generative': 0, 'stub': 1}\n",
      "\n",
      "Привет здарова\n",
      "{'intent': 1, 'generative': 0, 'stub': 1}\n",
      "\n",
      "Как жизнь? Прекрасный выбор для Вас!\n",
      "{'intent': 2, 'generative': 0, 'stub': 1}\n",
      "\n",
      "Что? Штож, я пытался это понять\n",
      "{'intent': 2, 'generative': 0, 'stub': 2}\n",
      "\n",
      "Эммм Не поняла Вас\n",
      "{'intent': 2, 'generative': 0, 'stub': 3}\n",
      "\n",
      "Отстань Идемте!..\n",
      "{'intent': 2, 'generative': 1, 'stub': 3}\n",
      "\n",
      "ывв ПОпробуйте еще раз!\n",
      "{'intent': 2, 'generative': 1, 'stub': 4}\n",
      "\n",
      "ыв Пожалуйста, повторите фразу\n",
      "{'intent': 2, 'generative': 1, 'stub': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
